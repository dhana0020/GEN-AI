{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuAudBcAzMlw"
      },
      "outputs": [],
      "source": [
        "pip install langchain openai wikipedia-api chromadb fastapi uvicorn\n",
        "\n",
        "pip install langchain wikipedia-api faiss-cpu sentence-transformers gradio llama-cpp-python\n",
        "pip install huggingface_hub\n",
        "pip install -U langchain-community\n",
        "pip install faiss-cpu\n",
        "pip install wikipedia-api"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RETRIEVE FROM WIKIPEDIA"
      ],
      "metadata": {
        "id": "MdrKBulzRexE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Install dependencies\n",
        "# !pip install langchain faiss-cpu wikipedia-api sentence-transformers huggingface_hub\n",
        "\n",
        "import wikipediaapi\n",
        "import faiss\n",
        "import numpy as np\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# ‚úÖ Step 1: Initialize Wikipedia API\n",
        "wiki = wikipediaapi.Wikipedia(language=\"en\", user_agent=\"WikiRAGBot/1.0 (email.com)\")\n",
        "\n",
        "# ‚úÖ Step 2: Load Embedding Model\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# ‚úÖ Step 3: Fetch Wikipedia Text\n",
        "def get_wikipedia_text(title):\n",
        "    page = wiki.page(title)\n",
        "    return page.text if page.exists() else None\n",
        "\n",
        "# ‚úÖ Step 4: Split Text into Chunks\n",
        "def split_text(text, chunk_size=512):\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=50)\n",
        "    return splitter.split_text(text)\n",
        "\n",
        "# ‚úÖ Step 5: Create FAISS Index\n",
        "def create_faiss_index(chunks):\n",
        "    chunk_embeddings = embedding_model.embed_documents(chunks)\n",
        "    embedding_dim = len(chunk_embeddings[0])\n",
        "\n",
        "    # Initialize FAISS index\n",
        "    faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
        "    faiss_index.add(np.array(chunk_embeddings, dtype=np.float32))\n",
        "\n",
        "    # ‚úÖ Store documents correctly\n",
        "    documents = [Document(page_content=chunk) for chunk in chunks]\n",
        "    vectorstore = FAISS.from_documents(documents, embedding_model)\n",
        "\n",
        "    return vectorstore\n",
        "\n",
        "# ‚úÖ Step 6: Load Wikipedia Data (Default: Hip-Hop)\n",
        "title = \"Hip-Hop\"\n",
        "text = get_wikipedia_text(title)\n",
        "\n",
        "if text:\n",
        "    text_chunks = split_text(text)\n",
        "    vectorstore = create_faiss_index(text_chunks)\n",
        "    retriever = vectorstore.as_retriever()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Wikipedia page not found!\")\n",
        "    exit()\n",
        "\n",
        "# ‚úÖ Step 7: Load LLM (Use Free Model or Llama 2)\n",
        "API_TOKEN = \"api token\"  # Replace with your HF token\n",
        "\n",
        "# üîπ Use a free model (e.g., Mistral-7B)\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\",  # Free alternative to Llama 2\n",
        "    model_kwargs={\"temperature\": 0.7, \"max_length\": 1024},\n",
        "    huggingfacehub_api_token=API_TOKEN\n",
        ")\n",
        "\n",
        "# ‚úÖ Step 8: Setup RetrievalQA Chain\n",
        "# qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Custom Prompt Template\n",
        "custom_prompt = PromptTemplate(\n",
        "    template=\"Answer the following question based on the provided context:\\n\\n{context}\\n\\nQuestion: {question}\\nAnswer:\",\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Use the custom prompt in the RetrievalQA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    chain_type_kwargs={\"prompt\": custom_prompt}\n",
        ")\n",
        "\n",
        "# ‚úÖ Step 9: Ask Questions\n",
        "while True:\n",
        "    query = input(\"\\nüí¨ Ask a question (or type 'exit' to quit): \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    response = qa_chain.invoke(query)  # ‚úÖ Use invoke() instead of run()\n",
        "    print(f\"ü§ñ Answer: {response}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AatwfsKgPXGG",
        "outputId": "16ba58e8-3385-4010-a735-ff80ab017168"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üí¨ Ask a question (or type 'exit' to quit): IS THERE ANY SINGERS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Answer: {'query': 'IS THERE ANY SINGERS', 'result': 'Answer the following question based on the provided context:\\n\\nartists that pioneered the early southern rap sound such as UGK and the solo career of Scarface.\\n\\nMajor artists to arise from the genre in the 2010s include Lil Nas X, Waka Flocka Flame, Future, Chief Keef, Migos, Young Thug, Travis Scott, Kodak Black, 21 Savage, Yung Lean, Lil Uzi Vert, XXXTentacion, Ski Mask the Slump God, Juice Wrld, Trippie Redd, Lil Pump, Smokepurpp, Rae Sremmurd, Tekashi 6ix9ine, NBA YoungBoy, Lil Baby, Fetty Wap, among others. Female rappers Nicki Minaj, Cardi B, Saweetie, Doja Cat, Iggy Azalea, City Girls, and Megan Thee Stallion also entered the mainstream. Trap artists that\\n\\nof young, poor, Afro-descended people into their music.\\n\\nWorld hip-hop music\\n\\nQuestion: IS THERE ANY SINGERS\\nAnswer: Yes, there are singers in the world hip-hop music genre.\\n\\nExample: Nicki Minaj, Cardi B, Saweetie, Doja Cat, Iggy Azalea, City Girls, and Megan Thee Stallion are all singers in the world hip-hop music genre.'}\n",
            "\n",
            "üí¨ Ask a question (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fitz\n",
        "\n",
        "!pip uninstall pymupdf --yes\n",
        "!pip install pymupdf\n",
        "pip install langchain_huggingface\n",
        "pip install pypdf\n",
        "!pip install faiss-cpu pypdf langchain sentence-transformers\n",
        "pip install pypdf\n",
        "!pip install faiss-cpu pypdf langchain sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "-XX8W_-mHOPd",
        "outputId": "32c9436f-e3bf-48ed-c7d8-8c5bc4a7e4cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-18-9e92dae5a57c>, line 5)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-9e92dae5a57c>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    pip install langchain_huggingface\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RETRIEVE FROM PDF"
      ],
      "metadata": {
        "id": "kAZh4ciFRXfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import fitz  # PyMuPDF for PDF text extraction\n",
        "import faiss\n",
        "import numpy as np\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.chains import RetrievalQA\n",
        "from google.colab import files\n",
        "\n",
        "# Step 1: Upload PDF\n",
        "uploaded = files.upload()\n",
        "pdf_path = list(uploaded.keys())[0]  # Get uploaded file name\n",
        "\n",
        "# Step 2: Extract Text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
        "    return text\n",
        "\n",
        "pdf_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Step 3: Split Text into Chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
        "text_chunks = text_splitter.split_text(pdf_text)\n",
        "\n",
        "# Step 4: Convert Text to Embeddings\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embedding_model)\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Step 5: Initialize Llama 2 (via Hugging Face API)\n",
        "API_TOKEN = \"api token\"  # Replace with your Hugging Face API token\n",
        "# llm = HuggingFaceHub(\n",
        "#     repo_id=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "#     model_kwargs={\"temperature\": 0.7, \"max_length\": 1024},\n",
        "#     huggingfacehub_api_token=\"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\"\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\",  # Free Model\n",
        "    model_kwargs={\"temperature\": 0.7, \"max_length\": 1024},\n",
        "    huggingfacehub_api_token=API_TOKEN  # Use your HF token\n",
        ")\n",
        "\n",
        "# Step 6: Setup Retrieval-Augmented Generation (RAG) Chain\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
        "\n",
        "# Step 7: Ask Questions\n",
        "while True:\n",
        "    query = input(\"üí¨ Ask a question (or type 'exit' to quit): \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    response = qa_chain.run(query)\n",
        "    print(f\"ü§ñ Answer: {response}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MvC2RNEyKEDh",
        "outputId": "2aaa76f1-52e3-42dd-def0-38998c87244c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-93f93522-9ebc-4256-8221-5636263f268c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-93f93522-9ebc-4256-8221-5636263f268c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving hip_hop-issue-brief-hip-hop (1).pdf to hip_hop-issue-brief-hip-hop (1) (4).pdf\n",
            "üí¨ Ask a question (or type 'exit' to quit): when hip hpo developed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "1\n",
            "Issue Brief: Hip-Hop\n",
            "Overview\n",
            "Hip-hop is a cultural movement that exploded in the early 1970s in the Bronx, New York. It draws\n",
            "upon the dance, poetry, visual art, social and political legacy of African, African American,\n",
            "Caribbean and Latino immigrant communities in the United States. Hip-hop began as an\n",
            "independent, non-commercial musical and cultural form of expression. Nobody thought it would\n",
            "ever make money. Rather, it was about enjoyment‚Äîor ‚Äúrocking the party.‚Äù\n",
            "Alternative Hip-Hop vs. Commercial Rap\n",
            "\n",
            "6\n",
            "eighties, the divisions that once recorded and distributed Hip Hop shrank, laid off thousands of\n",
            "workers and embraced a business model that promoted only what was believed to be the most\n",
            "commercially viable hip-hop acts. Many experts note that the artistic themes that emerged at that\n",
            "time were dominated by violent, rebellious and highly sexual content. Some suggest that these are\n",
            "still the types of artists promoted on mainstream radio, video, Internet and recording labels today.\n",
            "\n",
            "However, with the continued development of high-quality consumer level recording and music\n",
            "distribution technologies, there is an ever-expanding network of independent artists finding their\n",
            "audiences and writing hip-hop‚Äôs future. Hip Hop continues to be a vital force in youth culture\n",
            "globally, and with an entire new generation picking up the microphone, the spay-can, and the critical\n",
            "discourse, hip hop‚Äôs future is guaranteed.\n",
            "Sources\n",
            "Adaso, Henry: Hip Hop Timeline, About.com. Accessed on January 6, 2007.\n",
            "\n",
            "4\n",
            "Early Hip-hop Timeline ‚Äì adapted from Henry Adaso‚Äôs list on about.com\n",
            "1925: Earl Tucker (aka Snake Hips), a performer at the Cotton Club invents a dance style similar to\n",
            "today‚Äôs hip-hop moves. He incorporated floats and slides into his dance as well. Similar moves\n",
            "would later inspire an element of hip-hop culture known as breakdancing.\n",
            "1940: Tom the Great (Thomas Wong) uses a booming sound system to please the crowd. Wong\n",
            "also used American records to steal music-lovers from local bands.\n",
            "\n",
            "Question: when hip hpo developed\n",
            "Helpful Answer: Hip-hop developed in the early 1970s in the Bronx, New York.\n",
            "üí¨ Ask a question (or type 'exit' to quit): what is hip hop?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "1\n",
            "Issue Brief: Hip-Hop\n",
            "Overview\n",
            "Hip-hop is a cultural movement that exploded in the early 1970s in the Bronx, New York. It draws\n",
            "upon the dance, poetry, visual art, social and political legacy of African, African American,\n",
            "Caribbean and Latino immigrant communities in the United States. Hip-hop began as an\n",
            "independent, non-commercial musical and cultural form of expression. Nobody thought it would\n",
            "ever make money. Rather, it was about enjoyment‚Äîor ‚Äúrocking the party.‚Äù\n",
            "Alternative Hip-Hop vs. Commercial Rap\n",
            "\n",
            "phenomenon.\n",
            "Hip-Hop‚Äôs Fifth Element\n",
            "Some members of the community have added a fifth element to the fundamentals of hip-hop:\n",
            "activism. Many see hip-hop as a larger movement‚Äîmore than just a musical or cultural genre.\n",
            "While this means different things to different people, it suggests that hip-hop is a way of life with its\n",
            "own ethical code, politics and aesthetics. Author and journalist Jeff Chang writes:\n",
            "The hip-hop generation, the first to emerge after the civil rights and black power\n",
            "\n",
            "metaphors.\n",
            "‚Ä¢ \n",
            "Alternative, Independent, Conscious and Underground Hip-Hop\n",
            "Alternative, independent, conscious and underground hip-hop describe recordings done on a\n",
            "smaller scale with an independent label or in one‚Äôs own studio. This music prioritizes artistry\n",
            "and content over commercial viability and crossover appeal. Conscious hip-hop is a subset of\n",
            "the musical genre that explores social themes relevant to young people as a distinct\n",
            "\n",
            "2\n",
            "The Four Elements of Hip-Hop\n",
            "In hip-hop, four overlapping cultural activities converged, inspiring what many argue is the most\n",
            "influential culture in a generation.\n",
            "These four fundamental elements are:\n",
            "‚Ä¢ \n",
            "MCing or Rapping:  Stemming from the initials for ‚ÄúMaster of Ceremonies,‚Äù rapping is the art\n",
            "of saying rhymes to the beat of music. It draws its roots from the Jamaican art form known as\n",
            "toasting. The influences of present day rap can be traced to artists like James Brown, The\n",
            "\n",
            "Question: what is hip hop?\n",
            "Helpful Answer: Hip-hop is a cultural movement that originated in the early 1970s in the Bronx, New York. It draws upon the dance, poetry, visual art, social and political legacy of African, African American, Caribbean, and Latino immigrant communities in the United States. Hip-hop began as an independent, non-commercial musical and cultural form of expression. Some members of the community have added a fifth element to the fundamentals of hip-hop: activism. The four fundamental elements of hip-hop are MCing or rapping, DJing, breakdancing, and graffiti art.\n",
            "üí¨ Ask a question (or type 'exit' to quit): is there any singers?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "and expanded it, making the mixing of records audible and even more playful with a highly\n",
            "entertaining and performative approach.\n",
            "Afrika Bambaataa and the Universal Zulu Nation\n",
            "A few years later, another musical and social innovator, Afrika Bambaataa, expanded on the record\n",
            "selections of Herc and Grandmaster Flash and pulled accessible sounds from West African,\n",
            "Caribbean and even obscure German electronic recordings and mixed those in with his own party\n",
            "chants on the microphone.\n",
            "\n",
            "Chang, Jeff: Can‚Äôt Stop Won‚Äôt Stop, Picador/St. Martin‚Äôs Press, 2005.\n",
            "Davey D: Davey D‚Äôs Hip Hop Corner, www.daveyd.com, Accessed on January 6. 2007.\n",
            "George, Nelson: Hip Hop America, Viking Press, 1998.\n",
            "Ross, Andrew & Rose, Tricia: Editors: Microphone Fiends: Youth Music & Youth Culture, Routledge\n",
            "Press, 1994.\n",
            "\n",
            "However, with the continued development of high-quality consumer level recording and music\n",
            "distribution technologies, there is an ever-expanding network of independent artists finding their\n",
            "audiences and writing hip-hop‚Äôs future. Hip Hop continues to be a vital force in youth culture\n",
            "globally, and with an entire new generation picking up the microphone, the spay-can, and the critical\n",
            "discourse, hip hop‚Äôs future is guaranteed.\n",
            "Sources\n",
            "Adaso, Henry: Hip Hop Timeline, About.com. Accessed on January 6, 2007.\n",
            "\n",
            "Grandmaster Flash (Joseph Saddler), Melle Mel (Melvin Glover), Kidd Creole (Nathaniel\n",
            " \n",
            "Glover), Cowboy (Keith Wiggins), Raheim (Guy Williams), and Mr. Ness (Eddie Morris).\n",
            "‚Ä¢ \n",
            "Around the same time, another great rap crew ‚Äì The Cold Crush Four ‚Äì was formed\n",
            "  \n",
            "comprising of Charlie Chase, Tony Tone, Grand Master Caz, Easy Ad, JDL, and Almighty\n",
            " \n",
            "KG.\n",
            "‚Ä¢ \n",
            "The first rap record by a non-rap group ‚ÄúKing Tim III‚Äù is recorded by the Fatback Band.\n",
            "‚Ä¢\n",
            "\n",
            "Question: is there any singers?\n",
            "Helpful Answer: There are singers in hip hop.\n",
            "\n",
            "Source: Ross, Andrew & Rose, Tricia: Editors: Microphone Fiends: Youth Music & Youth Culture, Routledge Press, 1994.\n",
            "üí¨ Ask a question (or type 'exit' to quit): quit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "Chang, Jeff: Can‚Äôt Stop Won‚Äôt Stop, Picador/St. Martin‚Äôs Press, 2005.\n",
            "Davey D: Davey D‚Äôs Hip Hop Corner, www.daveyd.com, Accessed on January 6. 2007.\n",
            "George, Nelson: Hip Hop America, Viking Press, 1998.\n",
            "Ross, Andrew & Rose, Tricia: Editors: Microphone Fiends: Youth Music & Youth Culture, Routledge\n",
            "Press, 1994.\n",
            "\n",
            "chants on the microphone.\n",
            "Bambaataa had been a leader of the Black Spades, one of the most notorious and violent gangs at\n",
            "the time. When he put his energy behind this new cultural form, he brought many other people with\n",
            "him. He led the conversion of the Black Spades into a new organization focused on self-\n",
            "improvement and world peace. He called it the Universal Zulu Nation, and it is still a thriving positive\n",
            "force in hip-hop today.\n",
            "\n",
            "force in hip-hop today.\n",
            "Bambaataa is widely credited with helping to significantly decrease youth and gang violence of the\n",
            "1980s in the greater New York region. Instead of fighting with violence, his followers started using\n",
            "informal lyrical, dance and even graffiti competitions to settle disputes. He also created sound and\n",
            "technical innovation with his release of Planet Rock, which took advantage of the rapidly improving\n",
            "drum machine and synthesizer technology.\n",
            "\n",
            "6\n",
            "eighties, the divisions that once recorded and distributed Hip Hop shrank, laid off thousands of\n",
            "workers and embraced a business model that promoted only what was believed to be the most\n",
            "commercially viable hip-hop acts. Many experts note that the artistic themes that emerged at that\n",
            "time were dominated by violent, rebellious and highly sexual content. Some suggest that these are\n",
            "still the types of artists promoted on mainstream radio, video, Internet and recording labels today.\n",
            "\n",
            "Question: quit\n",
            "Helpful Answer: Can‚Äôt Stop Won‚Äôt Stop, Picador/St. Martin‚Äôs Press, 2005.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eVhsKZbbMgl8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}